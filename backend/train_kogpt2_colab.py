# -*- coding: utf-8 -*-
"""
Google Colabì—ì„œ koGPT2 ëª¨ë¸ì„ k-fold êµì°¨ ê²€ì¦ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ë°©ë²•:
1. Google Colabì—ì„œ ì´ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‚´ìš©ì„ ë³µì‚¬
2. Colab ì…€ì—ì„œ ì‹¤í–‰
3. ê° foldë§ˆë‹¤ í•™ìŠµëœ ëª¨ë¸ì„ Google Driveì— ì €ì¥í•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ
"""

import os
import time
from pathlib import Path
from typing import List, Dict
import torch
import numpy as np
from sklearn.model_selection import KFold
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset

# CUDA ë””ë²„ê¹… í™œì„±í™” (device-side assert ì˜¤ë¥˜ ë””ë²„ê¹…ìš©)
os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "1")

# ===== ì„¤ì • =====
MODEL_ID = "skt/kogpt2-base-v2"  # koGPT2 ëª¨ë¸ ID
OUTPUT_DIR = "./kogpt2_finetuned"  # í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
K_FOLDS = 5  # k-fold êµì°¨ ê²€ì¦ì˜ k ê°’ (ì—°ì‚°ëŸ‰ ì¤„ì´ë ¤ë©´ 3 ê¶Œì¥)
EPOCHS = 2  # í•™ìŠµ ì—í¬í¬ ìˆ˜ (ì—°ì‚°ëŸ‰ ì¤„ì´ë ¤ë©´ 1ë¡œ ì„¤ì •)
LEARNING_RATE = 5e-5  # í•™ìŠµë¥ 
BATCH_SIZE = 4  # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •: 2, 4, 8, 16)
GRADIENT_ACCUMULATION_STEPS = 4  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
MAX_DATA_SIZE = 100  # ì‚¬ìš©í•  ìµœëŒ€ ë°ì´í„° ê°œìˆ˜ (ì „ì²´ ì‚¬ìš©í•˜ë ¤ë©´ None)

# ===== ë””ë°”ì´ìŠ¤ ì„¤ì • =====
# FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ë©´ GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  CPUë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤
# ê¸°ë³¸ê°’ì€ False (GPU ì „ìš© í•™ìŠµ)
FORCE_CPU = False  # Trueë¡œ ì„¤ì •í•˜ë©´ CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰

# GPU ì „ìš© í•™ìŠµ ëª¨ë“œ: GPUë§Œ ì‚¬ìš©í•˜ê³  CPUë¡œ ìë™ ì „í™˜í•˜ì§€ ì•ŠìŒ
# âš ï¸ CUDA ì˜¤ë¥˜ê°€ ê³„ì† ë°œìƒí•˜ë©´ Falseë¡œ ë³€ê²½í•˜ê±°ë‚˜ FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”
GPU_ONLY = True  # True: GPUë§Œ ì‚¬ìš©, GPU ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨. False: GPU ì˜¤ë¥˜ ì‹œ CPUë¡œ ì „í™˜


def download_kpoem_data(max_size: int = 100) -> List[Dict]:
    """
    Hugging Faceì—ì„œ KPoeM ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        max_size: ë¡œë“œí•  ìµœëŒ€ ë°ì´í„° ê°œìˆ˜
    
    Returns:
        ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ {'text': ì›ë¬¸, 'poem': ì‹œ} í˜•ì‹)
    """
    print(f"\n{'='*80}")
    print(f"[KPoeM ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ]")
    print(f"  - ì†ŒìŠ¤: Hugging Face (AKS-DHLAB/KPoEM)")
    print(f"  - ìµœëŒ€ ê°œìˆ˜: {max_size}")
    print(f"{'='*80}\n")
    
    try:
        # KPoEM ë°ì´í„°ì…‹ ë¡œë“œ
        print(f"[ë‹¤ìš´ë¡œë“œ ì‹œë„] AKS-DHLAB/KPoEM...")
        dataset = load_dataset(
            "csv",
            data_files={
                "train": "hf://datasets/AKS-DHLAB/KPoEM/KPoEM_poem_dataset_v4.tsv"
            },
            delimiter="\t",
            encoding="utf-8",
            quoting=3,  # QUOTE_NONE
        )
        dataset = dataset["train"]
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(dataset)}ê°œ ìƒ˜í”Œ")
        
        # ë°ì´í„° í˜•ì‹ ë³€í™˜
        normalized_data = []
        for i, item in enumerate(dataset):
            if max_size and i >= max_size:
                break
            
            if 'text' in item and item['text']:
                poem_text = str(item['text']).strip()
                normalized_data.append({
                    'text': poem_text,
                    'poem': poem_text
                })
        
        print(f"âœ… {len(normalized_data)}ê°œ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
        return normalized_data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def extract_keywords_simple(text: str, max_keywords: int = 10) -> List[str]:
    """
    ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (Colabì—ì„œëŠ” ë³µì¡í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)
    ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‚¤ì›Œë“œ ì¶”ì¶œì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
    """
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ: ëª…ì‚¬ ìœ„ì£¼ë¡œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” í˜•íƒœì†Œ ë¶„ì„ í•„ìš”)
    # ì—¬ê¸°ì„œëŠ” í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë‚˜ëˆ„ê³  ê¸¸ì´ê°€ 2 ì´ìƒì¸ ê²ƒë§Œ ì„ íƒ
    words = text.split()
    keywords = [w for w in words if len(w) >= 2][:max_keywords]
    return keywords if keywords else ["ì‹œ", "ê°ì •"]


def classify_emotion_simple(text: str) -> Dict[str, str]:
    """
    ê°„ë‹¨í•œ ê°ì • ë¶„ë¥˜ (Colabì—ì„œëŠ” ë³µì¡í•œ ëª¨ë¸ ì—†ì´)
    ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ê°ì • ë¶„ë¥˜ê°€ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
    """
    # ê°„ë‹¨í•œ ê°ì • ë¶„ë¥˜: ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ê¸°ë°˜
    positive_words = ["ì¢‹", "í–‰ë³µ", "ê¸°ì¨", "ì‚¬ë‘", "í¬ë§", "ë°", "ë”°ëœ»"]
    negative_words = ["ìŠ¬", "ìš°ìš¸", "ì•„í””", "í˜ë“¦", "ì–´ë‘ ", "ì°¨ê°‘"]
    
    text_lower = text.lower()
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)
    
    if pos_count > neg_count:
        mood = "ë°ì€"
        emotion = "ê¸ì •"
    elif neg_count > pos_count:
        mood = "ì–´ë‘ìš´"
        emotion = "ë¶€ì •"
    else:
        mood = "ì”ì”í•œ"
        emotion = "ì¤‘ë¦½"
    
    return {'mood': mood, 'emotion': emotion}


def build_prompt_kogpt2(keywords: List[str], mood: str, lines: int, original_text: str) -> str:
    """
    koGPT2ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    kw_str = ", ".join(keywords[:10])
    
    prompt = f"""Write a Korean poem (í•œêµ­ì–´ ì‹œ) based on the keywords and mood below.

**CRITICAL: Language Requirement**
- You MUST write ONLY in Korean (Hangul, í•œê¸€).
- Do NOT use Chinese characters (í•œì), Japanese characters, English, or any other language.
- Use ONLY Korean characters (ê°€-í£) and Korean punctuation.
- The output MUST be a Korean poem.

**Output Requirements**
- Output ONLY the poem text (no title, no explanation, no keywords, no numbering).
- The output MUST be in poem form with line breaks.
- Write EXACTLY {lines} lines (one line per verse; no empty lines).

**Content**
- Keywords: {kw_str}
- Mood: {mood}
{f'**Original Prose (Context)**\n\"\"\"{original_text.strip()}\"\"\"\n' if original_text else ''}

**Style Rules (strict)**
1) Keep each line short and lyrical.
2) Show, don't tell.
3) Avoid plain narration and diary-like tone.
4) In Korean, avoid declarative endings like "~ë‹¤", "~ì´ë‹¤", "~í–ˆë‹¤".
5) Avoid explicit subjects/time markers like "ë‚˜ëŠ”", "ê·¸ëŠ”/ê·¸ë…€ëŠ”", "ì˜¤ëŠ˜ì€/ì–´ì œëŠ”".

Poem:
"""
    return prompt


def convert_poem_to_prose(poem: str) -> str:
    """
    ì‹œë¥¼ ì‚°ë¬¸(ì¼ìƒ ê¸€)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì‹œì˜ ì¤„ë°”ê¿ˆì„ ì œê±°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    
    Args:
        poem: ì‹œ í…ìŠ¤íŠ¸
    
    Returns:
        ì‚°ë¬¸ í…ìŠ¤íŠ¸
    """
    if not poem:
        return ""
    
    # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    lines = [line.strip() for line in poem.split('\n') if line.strip()]
    
    # ì‹œì˜ ê° ì¤„ì„ ì—°ê²°í•˜ì—¬ ì‚°ë¬¸ìœ¼ë¡œ ë§Œë“¤ê¸°
    # ì‹œì  í‘œí˜„ì„ ì¼ìƒì ì¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜
    prose = " ".join(lines)
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸° (í† í° ê¸¸ì´ ì œí•œì„ ìœ„í•´)
    if len(prose) > 400:  # ëŒ€ëµ 400ìë¡œ ì œí•œ
        prose = prose[:400] + "..."
    
    # ì‹œì  í‘œí˜„ì„ ì¼ìƒ í‘œí˜„ìœ¼ë¡œ ë³€í™˜
    # ì˜ˆ: "ê½ƒì²˜ëŸ¼" â†’ "ê½ƒê³¼ ê°™ì´", "ë³„ì²˜ëŸ¼" â†’ "ë³„ê³¼ ê°™ì´"
    prose = prose.replace("ì²˜ëŸ¼", "ê³¼ ê°™ì´")
    prose = prose.replace("ê°™ì´", "ì²˜ëŸ¼")
    
    # ë¬¸ì¥ ë¶€í˜¸ ì •ë¦¬
    if not prose.endswith(('.', '!', '?', 'ë‹¤', 'ìš”', '...')):
        prose += "."
    
    return prose


def prepare_training_data(train_data: List[Dict], tokenizer) -> List[Dict]:
    """
    í•™ìŠµ ë°ì´í„°ë¥¼ koGPT2 í•™ìŠµ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    1. ì‹œ ì›ë¬¸ë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì‹œì˜ í˜•ì‹/êµ¬ì¡°/í‘œí˜„ ë°©ì‹ì„ í•™ìŠµ
    2. ì‚°ë¬¸ â†’ ì‹œ ë³€í™˜ì„ í•™ìŠµí•˜ì—¬ ì‚°ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³  ì‹œë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµ
    
    Args:
        train_data: í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ {'text': ì›ë¬¸, 'poem': ì‹œ})
        tokenizer: koGPT2 í† í¬ë‚˜ì´ì €
    
    Returns:
        í•™ìŠµìš© ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
    """
    training_examples = []
    total = len(train_data)
    
    print(f"\n[í•™ìŠµ ë°ì´í„° ì¤€ë¹„]")
    print(f"  - ì´ {total}ê°œ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    print(f"  - í•™ìŠµ í˜•ì‹:")
    print(f"    1. ì‹œ ì›ë¬¸ í•™ìŠµ (ì‹œì˜ í˜•ì‹/êµ¬ì¡°/í‘œí˜„ ë°©ì‹ í•™ìŠµ)")
    print(f"    2. ì‚°ë¬¸ â†’ ì‹œ ë³€í™˜ í•™ìŠµ (ì‚°ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³  ì‹œ ìƒì„±)")
    
    poem_only_count = 0
    prose_to_poem_count = 0
    
    for idx, item in enumerate(train_data, 1):
        if idx % 10 == 0 or idx == total:
            print(f"  - ì§„í–‰ ì¤‘: {idx}/{total} ({idx*100//total}%)")
        
        poem = item.get('poem', '') or item.get('text', '')
        
        if not poem:
            continue
        
        # ===== 1. ì‹œ ì›ë¬¸ë§Œìœ¼ë¡œ í•™ìŠµ (ì‹œì˜ í˜•ì‹/êµ¬ì¡°/í‘œí˜„ ë°©ì‹ í•™ìŠµ) =====
        # ì‹œ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ í•™ìŠµí•˜ì—¬ koGPT2ê°€ ì‹œê°€ ë¬´ì—‡ì¸ì§€, ì‹œì˜ í˜•ì‹ì´ ì–´ë–¤ ê²ƒì¸ì§€ í•™ìŠµ
        # "ì‹œ: " ì—†ì´ ì‹œë§Œ í•™ìŠµí•˜ì—¬ "ì‹œ: " ë°˜ë³µ ìƒì„± íŒ¨í„´ ë°©ì§€
        # ëŒ€ì‹  ì‚°ë¬¸â†’ì‹œ ë³€í™˜ í•™ìŠµì—ì„œë§Œ "ì‹œ: " íŒ¨í„´ í•™ìŠµ
        poem_text = poem.strip()  # "ì‹œ: " ì œê±° - ë°˜ë³µ íŒ¨í„´ ë°©ì§€
        training_examples.append({
            'text': poem_text,
            'prose': '',  # ì‹œ ì›ë¬¸ë§Œ ìˆëŠ” ê²½ìš°
            'poem': poem.strip()
        })
        poem_only_count += 1
        
        # ===== 2. ì‚°ë¬¸ â†’ ì‹œ ë³€í™˜ í•™ìŠµ =====
        # ì‹œë¥¼ ì‚°ë¬¸ìœ¼ë¡œ ë³€í™˜ (ì¼ìƒ ê¸€ì²˜ëŸ¼ ë§Œë“¤ê¸°)
        # KPoeM ë°ì´í„°ì…‹ì—ëŠ” ì›ë¬¸ì´ ì—†ìœ¼ë¯€ë¡œ ì‹œë¥¼ ì‚°ë¬¸ìœ¼ë¡œ ë³€í™˜
        prose = convert_poem_to_prose(poem)
        
        if prose:
            # "ì‚°ë¬¸: [ì‚°ë¬¸ ë‚´ìš©]\nì‹œ: [ì‹œ ë‚´ìš©]" í˜•ì‹ìœ¼ë¡œ í•™ìŠµí•˜ì—¬
            # ëª¨ë¸ì´ "ì‚°ë¬¸: [ë‚´ìš©]"ì„ ì…ë ¥ë°›ìœ¼ë©´ ê·¸ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³ 
            # "ì‹œ: [ë‚´ìš©]"ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ
            full_text = f"ì‚°ë¬¸: {prose.strip()}\nì‹œ: {poem.strip()}"
            
            training_examples.append({
                'text': full_text,
                'prose': prose.strip(),
                'poem': poem.strip()
            })
            prose_to_poem_count += 1
    
    print(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(training_examples)}ê°œ í•™ìŠµ ì˜ˆì œ ìƒì„±")
    print(f"    - ì‹œ ì›ë¬¸ë§Œ í•™ìŠµ: {poem_only_count}ê°œ (\"ì‹œ: \" ì—†ì´ ì‹œë§Œ í•™ìŠµ)")
    print(f"    - ì‚°ë¬¸ â†’ ì‹œ ë³€í™˜ í•™ìŠµ: {prose_to_poem_count}ê°œ")
    print(f"  - í•™ìŠµ í˜•ì‹ ì˜ˆì‹œ:")
    if training_examples:
        # ì‹œ ì›ë¬¸ë§Œ ìˆëŠ” ì˜ˆì‹œ
        poem_example = next((ex for ex in training_examples if not ex['prose']), None)
        if poem_example:
            print(f"    [ì‹œ ì›ë¬¸ í•™ìŠµ] {poem_example['text'][:60]}...")
        
        # ì‚°ë¬¸ â†’ ì‹œ ë³€í™˜ ì˜ˆì‹œ
        prose_example = next((ex for ex in training_examples if ex['prose']), None)
        if prose_example:
            print(f"    [ì‚°ë¬¸â†’ì‹œ ë³€í™˜] ì…ë ¥(ì‚°ë¬¸): {prose_example['prose'][:40]}...")
            print(f"                  í•™ìŠµ í˜•ì‹: \"ì‚°ë¬¸: {prose_example['prose'][:30]}...\\nì‹œ: {prose_example['poem'][:30]}...\"")
    
    return training_examples


def train_kogpt2_model(
    train_data: List[Dict],
    output_dir: str,
    epochs: int = 2,
    learning_rate: float = 5e-5,
    batch_size: int = 4,
    gradient_accumulation_steps: int = 4
) -> str:
    """
    koGPT2 ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    
    Args:
        train_data: í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        output_dir: í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        batch_size: ë°°ì¹˜ í¬ê¸°
        gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
    
    Returns:
        í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    """
    print(f"\n{'='*80}")
    print(f"[koGPT2 ëª¨ë¸ í•™ìŠµ ì‹œì‘]")
    print(f"  - í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    print(f"  - Epochs: {epochs}")
    print(f"  - Learning Rate: {learning_rate}")
    print(f"  - Batch Size: {batch_size}")
    print(f"  - Gradient Accumulation Steps: {gradient_accumulation_steps}")
    print(f"{'='*80}\n")
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"[1/5] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©: {MODEL_ID}")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì´ì „ foldì˜ ì”ì—¬ ë©”ëª¨ë¦¬ ì œê±°)
    # GPUê°€ ì˜¤ì—¼ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    if torch.cuda.is_available() and os.environ.get('CUDA_VISIBLE_DEVICES') != '':
        try:
            # GPU ìƒíƒœ í™•ì¸ (ê°„ë‹¨í•œ ì—°ì‚°ìœ¼ë¡œ)
            _ = torch.cuda.get_device_name(0)
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        except Exception as e:
            print(f"  âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"  ğŸ’¡ GPUê°€ ì˜¤ì—¼ë˜ì—ˆìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)  # use_fastëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
    
    # pad_token ì„¤ì • ì „ vocab í¬ê¸° í™•ì¸
    initial_vocab_size = len(tokenizer)
    initial_base_vocab_size = tokenizer.vocab_size
    
    print(f"  - ì´ˆê¸° vocab í¬ê¸°: {initial_vocab_size} (base: {initial_base_vocab_size})")
    
    # ì´ˆê¸° ìƒíƒœì—ì„œ ì´ë¯¸ ì¶”ê°€ í† í°ì´ ìˆëŠ”ì§€ í™•ì¸
    if initial_vocab_size > initial_base_vocab_size:
        diff = initial_vocab_size - initial_base_vocab_size
        print(f"  âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œ ì´ë¯¸ ì¶”ê°€ í† í° {diff}ê°œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print(f"  ğŸ”§ í† í¬ë‚˜ì´ì €ë¥¼ base_vocab_sizeì— ë§ì¶° ì¡°ì •í•©ë‹ˆë‹¤...")
        
        # ì¶”ê°€ í† í°ì´ ìˆë‹¤ë©´, pad_token ì„¤ì • ì‹œ ìƒˆ í† í°ì„ ì¶”ê°€í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
        # í† í¬ë‚˜ì´ì €ì˜ vocab_sizeë¥¼ base_vocab_sizeë¡œ ì œí•œ
        # (ì‹¤ì œë¡œëŠ” ëª¨ë¸ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ ëª¨ë¸ì„ ë¦¬ì‚¬ì´ì¦ˆí•˜ëŠ” ê²ƒì´ ë” ì•ˆì „)
    
    # pad_token ì„¤ì • (ì¶”ê°€ í† í°ì„ ë§Œë“¤ì§€ ì•Šê³  eos_token ì¬ì‚¬ìš©)
    # koGPT2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ eos_tokenì„ ì¬ì‚¬ìš©
    if tokenizer.pad_token is None:
        # eos_tokenì„ pad_tokenìœ¼ë¡œ ì¬ì‚¬ìš© (ìƒˆ í† í° ì¶”ê°€ ì•ˆ í•¨)
        eos_token_id = tokenizer.eos_token_id
        eos_token = tokenizer.eos_token
        
        # ë°©ë²•: special_tokens_mapì„ ë¨¼ì € ìˆ˜ì •í•œ í›„ ì†ì„± ì„¤ì •
        # ì´ë ‡ê²Œ í•˜ë©´ add_special_tokensê°€ í˜¸ì¶œë˜ì§€ ì•ŠìŒ
        if hasattr(tokenizer, 'special_tokens_map'):
            # special_tokens_mapì— pad_tokenì„ eos_tokenìœ¼ë¡œ ë§¤í•‘
            original_map = tokenizer.special_tokens_map.copy()
            tokenizer.special_tokens_map['pad_token'] = eos_token
        
        # ì†ì„± ì§ì ‘ ì„¤ì • (add_special_tokens í˜¸ì¶œ ì•ˆ í•¨)
        tokenizer.pad_token = eos_token
        tokenizer.pad_token_id = eos_token_id
        
        # vocab í¬ê¸° ì¬í™•ì¸
        after_vocab_size = len(tokenizer)
        if after_vocab_size > initial_vocab_size:
            print(f"  âš ï¸ ê²½ê³ : pad_token ì„¤ì • í›„ vocab í¬ê¸°ê°€ ì¦ê°€í–ˆìŠµë‹ˆë‹¤!")
            print(f"     ì´ì „: {initial_vocab_size}, ì´í›„: {after_vocab_size}")
            print(f"  ğŸ”§ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ë” ì•ˆì „í•œ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤...")
            
            # í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ì‹œ ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)
            
            # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•: ì†ì„±ë§Œ ì§ì ‘ ì„¤ì • (special_tokens_map ìˆ˜ì • ì•ˆ í•¨)
            # ì´ë ‡ê²Œ í•˜ë©´ ìƒˆ í† í°ì´ ì¶”ê°€ë˜ì§€ ì•ŠìŒ
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            
            # vocab í¬ê¸° ìµœì¢… í™•ì¸
            final_vocab_size = len(tokenizer)
            if final_vocab_size > initial_vocab_size:
                print(f"  âš ï¸ ì—¬ì „íˆ vocab í¬ê¸°ê°€ ì¦ê°€í–ˆìŠµë‹ˆë‹¤: {final_vocab_size}")
                print(f"  ğŸ’¡ ì´ëŠ” í† í¬ë‚˜ì´ì € ìì²´ì— ì´ë¯¸ ì¶”ê°€ í† í°ì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.")
                print(f"  âœ… ëª¨ë¸ ë¦¬ì‚¬ì´ì¦ˆë¡œ ìë™ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
    else:
        print(f"  âœ… pad_tokenì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    
    # ìµœì¢… vocab í¬ê¸° í™•ì¸
    actual_vocab_size = len(tokenizer)  # ì‹¤ì œ í† í¬ë‚˜ì´ì € í¬ê¸° (ì¶”ê°€ í† í° í¬í•¨)
    base_vocab_size = tokenizer.vocab_size  # ê¸°ë³¸ vocab í¬ê¸°
    
    print(f"  - Base vocab size: {base_vocab_size}")
    print(f"  - Actual vocab size (len(tokenizer)): {actual_vocab_size}")
    print(f"  - Pad token ID: {tokenizer.pad_token_id}")
    print(f"  - EOS token ID: {tokenizer.eos_token_id}")
    
    if actual_vocab_size > base_vocab_size:
        diff = actual_vocab_size - base_vocab_size
        print(f"  âš ï¸ ì¶”ê°€ í† í°: {diff}ê°œ (ëª¨ë¸ ë¦¬ì‚¬ì´ì¦ˆ í•„ìš”)")
    
    # ë””ë°”ì´ìŠ¤ ì„ íƒ (CUDA > CPU)
    # FP16 gradient scaling ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ float32 ì‚¬ìš©
    device = "cpu"  # ê¸°ë³¸ê°’
    dtype = torch.float32
    
    # GPU ì „ìš© ëª¨ë“œ: FORCE_CPUê°€ Trueê°€ ì•„ë‹ˆë©´ GPUë§Œ ì‚¬ìš©
    if FORCE_CPU:
        print(f"  - ë””ë°”ì´ìŠ¤: CPU (FORCE_CPU=Trueë¡œ ê°•ì œ ì„¤ì •)")
        device = "cpu"
    elif not torch.cuda.is_available():
        if GPU_ONLY:
            raise RuntimeError(
                "âŒ GPUê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. GPU ì „ìš© ëª¨ë“œ(GPU_ONLY=True)ì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n"
                "ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                "   1. Colabì—ì„œ GPU ëŸ°íƒ€ì„ ì„ íƒ: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: GPU\n"
                "   2. GPU í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ì‹œê°„ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\n"
                "   3. CPU ëª¨ë“œë¡œ í•™ìŠµí•˜ë ¤ë©´ ì½”ë“œ ìƒë‹¨ì—ì„œ FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”."
            )
        else:
            print(f"  - ë””ë°”ì´ìŠ¤: CPU (GPU ì—†ìŒ)")
            device = "cpu"
    else:
        # GPU ì‚¬ìš© ê°€ëŠ¥
        try:
            # GPU ìƒíƒœ í™•ì¸ (ìµœì†Œí•œìœ¼ë¡œë§Œ)
            device_count = torch.cuda.device_count()
            if device_count > 0:
                device = "cuda"
                print(f"  - ë””ë°”ì´ìŠ¤: CUDA")
                # GPU ì •ë³´ ì¶œë ¥ (ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥í•˜ë¯€ë¡œ ì•ˆì „í•˜ê²Œ)
                try:
                    gpu_name = torch.cuda.get_device_name(0)
                    gpu_props = torch.cuda.get_device_properties(0)
                    print(f"  - GPU ì´ë¦„: {gpu_name}")
                    print(f"  - GPU ë©”ëª¨ë¦¬: {gpu_props.total_memory / (1024**3):.1f}GB")
                except:
                    pass  # GPU ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                print(f"  - ë°ì´í„° íƒ€ì…: float32 (FP16 gradient scaling ë¬¸ì œ ë°©ì§€)")
            else:
                if GPU_ONLY:
                    raise RuntimeError("GPU ë””ë°”ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. GPU ì „ìš© ëª¨ë“œì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                else:
                    device = "cpu"
                    print(f"  - ë””ë°”ì´ìŠ¤: CPU (GPU ë””ë°”ì´ìŠ¤ ì—†ìŒ)")
        except Exception as gpu_error:
            if GPU_ONLY:
                raise RuntimeError(
                    f"âŒ GPU ì‚¬ìš© ë¶ˆê°€: {gpu_error}\n"
                    "ğŸ’¡ GPU ì „ìš© ëª¨ë“œ(GPU_ONLY=True)ì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n"
                    "ğŸ”§ í•´ê²° ë°©ë²•:\n"
                    "   1. Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\n"
                    "   2. CPU ëª¨ë“œë¡œ í•™ìŠµí•˜ë ¤ë©´ ì½”ë“œ ìƒë‹¨ì—ì„œ FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”."
                )
            else:
                print(f"  âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: {gpu_error}")
                print(f"  ğŸ’¡ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                device = "cpu"
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    # ëª¨ë¸ ë¡œë”© (CPUì—ì„œ ë¨¼ì € ë¡œë“œí•˜ì—¬ ë¦¬ì‚¬ì´ì¦ˆ í›„ GPUë¡œ ì´ë™)
    # GPUë¡œ ë°”ë¡œ ì´ë™í•˜ë©´ ë¦¬ì‚¬ì´ì¦ˆ ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
    print(f"  - ëª¨ë¸ ë¡œë”© ì¤‘ (CPUì—ì„œ ë¨¼ì € ë¡œë“œ)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=dtype
    )
    
    # ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ CPUì— ìœ ì§€ (ë¦¬ì‚¬ì´ì¦ˆ ì „ê¹Œì§€)
    model = model.cpu()
    
    # loss_type ê²½ê³  í•´ê²°
    if hasattr(model.config, 'loss_type') and model.config.loss_type is None:
        try:
            if hasattr(model.config, '__dict__'):
                if 'loss_type' in model.config.__dict__:
                    delattr(model.config, 'loss_type')
        except:
            pass
    
    # ===== ì¤‘ìš”: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ì˜ vocab_size ì¼ì¹˜ í™•ì¸ ë° ìˆ˜ì • =====
    # pad_token ì„¤ì • ì „ì— ì´ˆê¸° í¬ê¸° í™•ì¸
    model_vocab_size_initial = model.config.vocab_size
    tokenizer_vocab_size_initial = len(tokenizer)  # ì´ˆê¸° í† í¬ë‚˜ì´ì € í¬ê¸°
    tokenizer_base_vocab_size = tokenizer.vocab_size  # í† í¬ë‚˜ì´ì € ê¸°ë³¸ í¬ê¸°
    
    print(f"\n  ğŸ“Š Vocab í¬ê¸° ë¹„êµ (ì´ˆê¸°):")
    print(f"     - ëª¨ë¸ vocab_size: {model_vocab_size_initial}")
    print(f"     - í† í¬ë‚˜ì´ì € base vocab_size: {tokenizer_base_vocab_size}")
    print(f"     - í† í¬ë‚˜ì´ì € ì‹¤ì œ í¬ê¸° (len): {tokenizer_vocab_size_initial}")
    
    # pad_token ì„¤ì • (ìƒˆ í† í° ì¶”ê°€í•˜ì§€ ì•Šê³  eos_token ì¬ì‚¬ìš©)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # pad_token ì„¤ì • í›„ í¬ê¸° í™•ì¸
    tokenizer_vocab_size_after = len(tokenizer)
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í¬ê¸° ë¶ˆì¼ì¹˜ í™•ì¸ ë° ìˆ˜ì • (ê°•ì œ)
    # ì´ˆê¸°ë¶€í„° ë¶ˆì¼ì¹˜í•˜ê±°ë‚˜ pad_token ì„¤ì • í›„ ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
    needs_resize = False
    target_vocab_size = tokenizer_vocab_size_after  # pad_token ì„¤ì • í›„ í¬ê¸° ì‚¬ìš©
    
    if model_vocab_size_initial != tokenizer_vocab_size_initial:
        # ì´ˆê¸°ë¶€í„° ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½ìš°
        print(f"  âš ï¸ Vocab í¬ê¸° ë¶ˆì¼ì¹˜ ê°ì§€ (ì´ˆê¸°):")
        print(f"     ëª¨ë¸: {model_vocab_size_initial}, í† í¬ë‚˜ì´ì €: {tokenizer_vocab_size_initial}")
        needs_resize = True
        target_vocab_size = tokenizer_vocab_size_initial
    
    if tokenizer_vocab_size_after > tokenizer_vocab_size_initial:
        # pad_token ì„¤ì •ìœ¼ë¡œ í¬ê¸°ê°€ ì¦ê°€í•œ ê²½ìš°
        print(f"  âš ï¸ pad_token ì„¤ì •ìœ¼ë¡œ í† í¬ë‚˜ì´ì € í¬ê¸°ê°€ ì¦ê°€í–ˆìŠµë‹ˆë‹¤:")
        print(f"     ì´ì „: {tokenizer_vocab_size_initial} â†’ ì´í›„: {tokenizer_vocab_size_after}")
        needs_resize = True
        target_vocab_size = tokenizer_vocab_size_after
    
    if model_vocab_size_initial != target_vocab_size:
        # ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°
        needs_resize = True
    
    if needs_resize:
        print(f"  ğŸ”§ ëª¨ë¸ ì„ë² ë”© ë ˆì´ì–´ë¥¼ í† í¬ë‚˜ì´ì € í¬ê¸°({target_vocab_size})ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤...")
        
        # ë¦¬ì‚¬ì´ì¦ˆ ì „ì— ëª¨ë¸ì´ CPUì— ìˆëŠ”ì§€ í™•ì¸
        if next(model.parameters()).is_cuda:
            print(f"  âš ï¸ ëª¨ë¸ì´ GPUì— ìˆìŠµë‹ˆë‹¤. CPUë¡œ ì´ë™ í›„ ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤...")
            model = model.cpu()
        
        # CPUì—ì„œ ë¦¬ì‚¬ì´ì¦ˆ ìˆ˜í–‰
        model.resize_token_embeddings(target_vocab_size)
        model_vocab_size_after = model.config.vocab_size
        print(f"  âœ… ëª¨ë¸ vocab_size ì—…ë°ì´íŠ¸: {model_vocab_size_initial} â†’ {model_vocab_size_after}")
        
        # ë¦¬ì‚¬ì´ì¦ˆ í›„ ì¦‰ì‹œ í™•ì¸
        if model_vocab_size_after != target_vocab_size:
            raise ValueError(
                f"âŒ ëª¨ë¸ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨!\n"
                f"   ëª©í‘œ: {target_vocab_size}, ì‹¤ì œ: {model_vocab_size_after}\n"
                f"   ì´ ë¶ˆì¼ì¹˜ê°€ CUDA ì˜¤ë¥˜ì˜ ì›ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        
        # ë¦¬ì‚¬ì´ì¦ˆ í›„ ëª¨ë¸ì´ ì—¬ì „íˆ CPUì— ìˆëŠ”ì§€ í™•ì¸
        if next(model.parameters()).is_cuda:
            print(f"  âš ï¸ ë¦¬ì‚¬ì´ì¦ˆ í›„ ëª¨ë¸ì´ GPUì— ìˆìŠµë‹ˆë‹¤. CPUë¡œ ì´ë™í•©ë‹ˆë‹¤...")
            model = model.cpu()
    else:
        model_vocab_size_after = model_vocab_size_initial
        print(f"  âœ… Vocab í¬ê¸° ì¼ì¹˜ í™•ì¸ (ë¦¬ì‚¬ì´ì¦ˆ ë¶ˆí•„ìš”)")
        
        # ëª¨ë¸ì´ CPUì— ìˆëŠ”ì§€ í™•ì¸
        if next(model.parameters()).is_cuda:
            print(f"  âš ï¸ ëª¨ë¸ì´ GPUì— ìˆìŠµë‹ˆë‹¤. CPUë¡œ ì´ë™í•©ë‹ˆë‹¤...")
            model = model.cpu()
    
    # ìµœì¢… í¬ê¸° í™•ì¸
    final_model_vocab_size = model.config.vocab_size
    final_tokenizer_vocab_size = len(tokenizer)
    
    print(f"\n  ğŸ“Š Vocab í¬ê¸° ë¹„êµ (ìµœì¢…):")
    print(f"     - ëª¨ë¸ vocab_size: {final_model_vocab_size}")
    print(f"     - í† í¬ë‚˜ì´ì € ì‹¤ì œ í¬ê¸° (len): {final_tokenizer_vocab_size}")
    
    # ìµœì¢… í™•ì¸: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    if final_model_vocab_size != final_tokenizer_vocab_size:
        print(f"  âŒ ìµœì¢… í™•ì¸ ì‹¤íŒ¨: vocab_size ë¶ˆì¼ì¹˜!")
        print(f"     ëª¨ë¸: {final_model_vocab_size}, í† í¬ë‚˜ì´ì €: {final_tokenizer_vocab_size}")
        raise ValueError(
            f"âŒ Vocab í¬ê¸° ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\n"
            f"   ëª¨ë¸: {final_model_vocab_size}, í† í¬ë‚˜ì´ì €: {final_tokenizer_vocab_size}\n"
            f"   ì´ ë¶ˆì¼ì¹˜ê°€ CUDA ì˜¤ë¥˜ì˜ ì›ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    
    # pad_token_idê°€ vocab_size ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
    pad_token_id = tokenizer.pad_token_id
    if pad_token_id >= final_model_vocab_size or pad_token_id < 0:
        print(f"  âš ï¸ ê²½ê³ : pad_token_id({pad_token_id})ê°€ vocab_size({final_model_vocab_size}) ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
        print(f"  ğŸ”§ eos_token_idë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        pad_token_id = tokenizer.pad_token_id
        if pad_token_id >= final_model_vocab_size or pad_token_id < 0:
            raise ValueError(f"eos_token_id({pad_token_id})ë„ vocab_size({final_model_vocab_size}) ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
    
    print(f"  âœ… Vocab í¬ê¸° ì™„ì „ ì¼ì¹˜ í™•ì¸!")
    print(f"  ğŸ“Œ ìµœì¢… vocab_size: {final_model_vocab_size} (ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì™„ì „ ì¼ì¹˜)")
    print(f"  ğŸ“Œ pad_token_id: {pad_token_id} (ìœ íš¨ ë²”ìœ„ ë‚´)\n")
    
    # ì‹¤ì œ ì‚¬ìš©í•  vocab_sizeëŠ” ëª¨ë¸ì˜ vocab_size (ë¦¬ì‚¬ì´ì¦ˆ í›„)
    safe_vocab_size = final_model_vocab_size
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª¨ë¸ ì´ë™ ì „)
    if device == "cuda" and torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        except:
            pass  # GPU ì •ë¦¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œ í›„ GPUë¡œ ì´ë™)
    # ë¦¬ì‚¬ì´ì¦ˆëŠ” CPUì—ì„œ ì™„ë£Œí–ˆìœ¼ë¯€ë¡œ ì´ì œ GPUë¡œ ì´ë™ ê°€ëŠ¥
    print(f"  - ëª¨ë¸ì„ {device.upper()}ë¡œ ì´ë™ ì¤‘...")
    try:
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™í•˜ê¸° ì „ì— í•œ ë²ˆ ë” í™•ì¸
        if device == "cuda":
            # GPUê°€ ì •ìƒì¸ì§€ í™•ì¸
            if not torch.cuda.is_available():
                raise RuntimeError("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # ëª¨ë¸ ì´ë™
            model = model.to(device)
            
            # GPU ì‚¬ìš© ì„±ê³µ ì‹œ GPU ì •ë³´ ì¶œë ¥
            try:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_props = torch.cuda.get_device_properties(0)
                print(f"  - GPU ì´ë¦„: {gpu_name}")
                print(f"  - GPU ë©”ëª¨ë¦¬: {gpu_props.total_memory / (1024**3):.1f}GB")
            except:
                pass  # GPU ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        else:
            # CPU ëª¨ë“œ
            model = model.to(device)
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
    except Exception as e:
        error_msg = str(e)
        if "CUDA" in error_msg or "cuda" in error_msg.lower() or "device-side assert" in error_msg.lower():
            # GPU ì˜¤ë¥˜ ë°œìƒ ì‹œ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
            if device == "cuda":
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                except:
                    pass
            
            if GPU_ONLY and not FORCE_CPU:
                # GPU ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ìì„¸í•œ ì•ˆë‚´
                raise RuntimeError(
                    f"âŒ GPUë¡œ ëª¨ë¸ ì´ë™ ì‹¤íŒ¨: {error_msg}\n"
                    "\n"
                    "ğŸ’¡ CUDAë€?\n"
                    "   - CUDAëŠ” NVIDIA GPUë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í”Œë«í¼ì…ë‹ˆë‹¤\n"
                    "   - Colabì—ì„œ GPU ëŸ°íƒ€ì„ì„ ì„ íƒí•˜ë©´ ìë™ìœ¼ë¡œ CUDAê°€ ì‚¬ìš©ë©ë‹ˆë‹¤\n"
                    "   - GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë ¤ë©´ CPU ëŸ°íƒ€ì„ì„ ì„ íƒí•˜ì„¸ìš”\n"
                    "\n"
                    "ğŸ”§ í•´ê²° ë°©ë²•:\n"
                    "   ë°©ë²• 1 (ê¶Œì¥): Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘\n"
                    "      ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\n"
                    "      (GPU ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ì˜¤ë¥˜ë¥¼ í•´ê²°)\n"
                    "\n"
                    "   ë°©ë²• 2: CPU ëª¨ë“œë¡œ í•™ìŠµ (ëŠë¦¬ì§€ë§Œ ì•ˆì •ì )\n"
                    "      ì½”ë“œ ìƒë‹¨ì—ì„œ ë‹¤ìŒì„ ë³€ê²½:\n"
                    "      FORCE_CPU = True  # False â†’ Trueë¡œ ë³€ê²½\n"
                    "\n"
                    "   ë°©ë²• 3: GPU ì „ìš© ëª¨ë“œ í•´ì œ\n"
                    "      ì½”ë“œ ìƒë‹¨ì—ì„œ ë‹¤ìŒì„ ë³€ê²½:\n"
                    "      GPU_ONLY = False  # True â†’ Falseë¡œ ë³€ê²½\n"
                    "      (GPU ì˜¤ë¥˜ ì‹œ ìë™ìœ¼ë¡œ CPUë¡œ ì „í™˜)\n"
                    "\n"
                    "   ë°©ë²• 4: CPU ëŸ°íƒ€ì„ ì‚¬ìš©\n"
                    "      ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: None\n"
                    "      (GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•ŠìŒ)"
                )
            else:
                print(f"  âš ï¸ GPUë¡œ ëª¨ë¸ ì´ë™ ì‹¤íŒ¨: {error_msg}")
                print(f"  ğŸ’¡ CPU ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
                device = "cpu"
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
                model = model.cpu()  # ì´ë¯¸ CPUì— ìˆì„ ìˆ˜ ìˆì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
                print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (CPU ëª¨ë“œ)\n")
        else:
            raise  # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
    
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    print(f"[2/5] í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    training_examples = prepare_training_data(train_data, tokenizer)
    print(f"âœ… {len(training_examples)}ê°œ í•™ìŠµ ì˜ˆì œ ì¤€ë¹„ ì™„ë£Œ\n")
    
    if len(training_examples) == 0:
        raise ValueError("í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. poem í•„ë“œê°€ ìˆëŠ” ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"[3/5] ë°ì´í„°ì…‹ ë³€í™˜ ì¤‘...")
    def tokenize_function(examples):
        # í† í¬ë‚˜ì´ì¦ˆ (paddingì€ DataCollatorì—ì„œ ì²˜ë¦¬)
        tokenized = tokenizer(
            examples['text'],
            truncation=True,
            max_length=512,
            padding=False  # paddingì„ Falseë¡œ ë³€ê²½ - DataCollatorì—ì„œ ì²˜ë¦¬
        )
        
        return tokenized
    
    dataset = Dataset.from_list(training_examples)
    # remove_columnsì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì œê±°
    columns_to_remove = ['text', 'prose', 'poem']
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
    existing_columns = set(dataset.column_names)
    columns_to_remove = [col for col in columns_to_remove if col in existing_columns]
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=columns_to_remove)
    
    # í† í°í™”ëœ ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì • (ë” ì² ì €í•˜ê²Œ)
    print(f"  - í† í°í™”ëœ ë°ì´í„° ê²€ì¦ ì¤‘...")
    # ëª¨ë¸ì˜ vocab_sizeë¥¼ ì‚¬ìš© (ë¦¬ì‚¬ì´ì¦ˆ í›„ì˜ ì‹¤ì œ í¬ê¸°)
    vocab_size = model.config.vocab_size  # ëª¨ë¸ ê¸°ì¤€ vocab_size ì‚¬ìš©
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    print(f"  - Vocab size (ëª¨ë¸ ê¸°ì¤€): {vocab_size}")
    print(f"  - Pad token ID: {pad_token_id}")
    print(f"  - EOS token ID: {tokenizer.eos_token_id}")
    
    # ì•ˆì „ì„± í™•ì¸: pad_token_idê°€ vocab_size ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
    if pad_token_id >= vocab_size or pad_token_id < 0:
        print(f"  âš ï¸ ê²½ê³ : pad_token_id({pad_token_id})ê°€ vocab_size({vocab_size}) ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
        print(f"  ğŸ”§ eos_token_idë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
        pad_token_id = tokenizer.eos_token_id
        if pad_token_id >= vocab_size or pad_token_id < 0:
            raise ValueError(f"eos_token_id({pad_token_id})ë„ vocab_size({vocab_size}) ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
    
    # ì˜ëª»ëœ í† í° ID ìˆ˜ì • í•¨ìˆ˜ (ë” ì•ˆì „í•˜ê²Œ)
    def fix_token_ids(example):
        if 'input_ids' in example:
            ids = example['input_ids']
            # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            ids_array = np.array(ids, dtype=np.int64)
            
            # ìŒìˆ˜ë‚˜ vocab_sizeë¥¼ ì´ˆê³¼í•˜ëŠ” ê°’ ìˆ˜ì •
            # pad_token_idë¡œ ëŒ€ì²´ (ë” ì•ˆì „)
            invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
            if np.any(invalid_mask):
                ids_array[invalid_mask] = pad_token_id
            
            example['input_ids'] = ids_array.tolist()
        
        # attention_maskë„ í™•ì¸
        if 'attention_mask' in example:
            mask = np.array(example['attention_mask'], dtype=np.int64)
            # attention_maskëŠ” 0 ë˜ëŠ” 1ë§Œ ê°€ëŠ¥
            mask = np.clip(mask, 0, 1)
            example['attention_mask'] = mask.tolist()
        
        return example
    
    # ëª¨ë“  ì˜ˆì œì— ëŒ€í•´ í† í° ID ê²€ì¦ ë° ìˆ˜ì •
    print(f"  - í† í° ID ê²€ì¦ ë° ìˆ˜ì • ì¤‘...")
    tokenized_dataset = tokenized_dataset.map(fix_token_ids, desc="í† í° ID ê²€ì¦")
    
    # ì „ì²´ ë°ì´í„°ì…‹ ê²€ì¦ (ëª¨ë“  ìƒ˜í”Œ ê²€ì‚¬ - 100ê°œ ì œí•œ ì œê±°)
    print(f"  - ì „ì²´ ë°ì´í„°ì…‹ ìµœì¢… ê²€ì¦ ì¤‘... (ì´ {len(tokenized_dataset)}ê°œ ìƒ˜í”Œ)")
    invalid_count = 0
    total_invalid_tokens = 0
    
    # ëª¨ë“  ìƒ˜í”Œ ê²€ì‚¬ (100ê°œ ì œí•œ ì œê±°)
    for i in range(len(tokenized_dataset)):
        sample = tokenized_dataset[i]
        if 'input_ids' in sample:
            input_ids = sample['input_ids']
            if input_ids:
                ids_array = np.array(input_ids, dtype=np.int64)
                invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
                
                if np.any(invalid_mask):
                    invalid_count += 1
                    invalid_token_count = np.sum(invalid_mask)
                    total_invalid_tokens += invalid_token_count
                    
                    # ë¬¸ì œê°€ ìˆëŠ” ìƒ˜í”Œ ìˆ˜ì •
                    ids_array[invalid_mask] = pad_token_id
                    tokenized_dataset[i]['input_ids'] = ids_array.tolist()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (100ê°œë§ˆë‹¤)
        if (i + 1) % 100 == 0 or (i + 1) == len(tokenized_dataset):
            print(f"    ì§„í–‰ ì¤‘: {i + 1}/{len(tokenized_dataset)} (ì˜ëª»ëœ ìƒ˜í”Œ: {invalid_count}ê°œ)")
    
    if invalid_count > 0:
        print(f"  âš ï¸ {invalid_count}ê°œ ìƒ˜í”Œì—ì„œ ì˜ëª»ëœ í† í° ID ë°œê²¬ ë° ìˆ˜ì • (ì´ {total_invalid_tokens}ê°œ í† í°)")
    else:
        print(f"  âœ… ëª¨ë“  ìƒ˜í”Œ ê²€ì¦ ì™„ë£Œ (ì˜ëª»ëœ í† í° ì—†ìŒ)")
    
    # ìµœì¢… ìƒ˜í”Œ ê²€ì¦ (ì—¬ëŸ¬ ìƒ˜í”Œ í™•ì¸)
    if len(tokenized_dataset) > 0:
        print(f"  - ìµœì¢… ìƒ˜í”Œ ê²€ì¦ ì¤‘...")
        sample_count = min(10, len(tokenized_dataset))  # ìµœëŒ€ 10ê°œ ìƒ˜í”Œ í™•ì¸
        all_valid = True
        
        for i in range(sample_count):
            sample = tokenized_dataset[i]
            if 'input_ids' in sample:
                input_ids = sample['input_ids']
                if input_ids:
                    ids_array = np.array(input_ids, dtype=np.int64)
                    max_id = np.max(ids_array) if len(ids_array) > 0 else 0
                    min_id = np.min(ids_array) if len(ids_array) > 0 else 0
                    
                    if max_id >= vocab_size or min_id < 0:
                        all_valid = False
                        print(f"  âš ï¸ ìƒ˜í”Œ {i}: ë²”ìœ„ ì´ˆê³¼ (min: {min_id}, max: {max_id}, vocab_size: {vocab_size})")
                        # ê°•ì œë¡œ ìˆ˜ì •
                        ids_array = np.clip(ids_array, 0, vocab_size - 1)
                        invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
                        if np.any(invalid_mask):
                            ids_array[invalid_mask] = pad_token_id
                        tokenized_dataset[i]['input_ids'] = ids_array.tolist()
        
        if all_valid:
            print(f"  âœ… ìµœì¢… ê²€ì¦ ì™„ë£Œ: ëª¨ë“  ìƒ˜í”Œì´ ìœ íš¨í•œ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤")
        else:
            print(f"  âš ï¸ ì¼ë¶€ ìƒ˜í”Œì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ê²€ì¦í•©ë‹ˆë‹¤...")
            # í•œ ë²ˆ ë” ì „ì²´ ê²€ì¦
            for i in range(len(tokenized_dataset)):
                sample = tokenized_dataset[i]
                if 'input_ids' in sample:
                    input_ids = sample['input_ids']
                    if input_ids:
                        ids_array = np.array(input_ids, dtype=np.int64)
                        ids_array = np.clip(ids_array, 0, vocab_size - 1)
                        invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
                        if np.any(invalid_mask):
                            ids_array[invalid_mask] = pad_token_id
                        tokenized_dataset[i]['input_ids'] = ids_array.tolist()
            print(f"  âœ… ì „ì²´ ë°ì´í„°ì…‹ ì¬ê²€ì¦ ì™„ë£Œ")
    
    print(f"âœ… ë°ì´í„°ì…‹ ë³€í™˜ ì™„ë£Œ: {len(tokenized_dataset)}ê°œ\n")
    
    # Data Collator ì„¤ì • (ì•ˆì „í•œ ì»¤ìŠ¤í…€ ë²„ì „)
    class SafeDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):
        """í† í° ID ê²€ì¦ì´ í¬í•¨ëœ ì•ˆì „í•œ Data Collator"""
        def __init__(self, tokenizer, mlm=False, model_vocab_size=None):
            super().__init__(tokenizer=tokenizer, mlm=mlm)
            # ëª¨ë¸ì˜ vocab_sizeë¥¼ ì‚¬ìš© (í† í¬ë‚˜ì´ì €ì˜ vocab_sizeê°€ ì•„ë‹Œ)
            if model_vocab_size is None:
                raise ValueError("model_vocab_sizeëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            self.model_vocab_size = model_vocab_size
            self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
            
            # pad_token_idê°€ vocab_size ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
            if self.pad_token_id >= self.model_vocab_size or self.pad_token_id < 0:
                print(f"  âš ï¸ ê²½ê³ : pad_token_id({self.pad_token_id})ê°€ vocab_size({self.model_vocab_size}) ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
                self.pad_token_id = tokenizer.eos_token_id
                if self.pad_token_id >= self.model_vocab_size or self.pad_token_id < 0:
                    raise ValueError(f"eos_token_id({self.pad_token_id})ë„ vocab_size({self.model_vocab_size}) ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
        
        def __call__(self, features):
            # ê¸°ë³¸ collator í˜¸ì¶œ ì „ì— í† í° ID ê²€ì¦ (ë§¤ìš° ê°•ë ¥í•˜ê²Œ - ëª¨ë“  í† í°ì„ ë¬´ì¡°ê±´ í´ë¦¬í•‘)
            vocab_size = self.model_vocab_size  # ëª¨ë¸ ê¸°ì¤€ vocab_size ì‚¬ìš©
            
            for feature in features:
                if 'input_ids' in feature:
                    ids = feature['input_ids']
                    if isinstance(ids, list):
                        ids_array = np.array(ids, dtype=np.int64)
                        # ëª¨ë“  í† í°ì„ ë¬´ì¡°ê±´ ë²”ìœ„ë¡œ í´ë¦¬í•‘ (ì•ˆì „í•˜ê²Œ)
                        ids_array = np.clip(ids_array, 0, vocab_size - 1)
                        # ì—¬ì „íˆ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” í† í°ì´ ìˆìœ¼ë©´ pad_token_idë¡œ êµì²´
                        invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
                        if np.any(invalid_mask):
                            ids_array[invalid_mask] = self.pad_token_id
                        feature['input_ids'] = ids_array.tolist()
                    elif isinstance(ids, torch.Tensor):
                        # Tensorì¸ ê²½ìš°ë„ ì²˜ë¦¬
                        ids_array = ids.cpu().numpy() if ids.is_cuda else ids.numpy()
                        # ëª¨ë“  í† í°ì„ ë¬´ì¡°ê±´ ë²”ìœ„ë¡œ í´ë¦¬í•‘ (ì•ˆì „í•˜ê²Œ)
                        ids_array = np.clip(ids_array, 0, vocab_size - 1)
                        # ì—¬ì „íˆ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” í† í°ì´ ìˆìœ¼ë©´ pad_token_idë¡œ êµì²´
                        invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
                        if np.any(invalid_mask):
                            ids_array[invalid_mask] = self.pad_token_id
                        feature['input_ids'] = torch.tensor(ids_array, dtype=ids.dtype, device=ids.device)
            
            # ê¸°ë³¸ collator í˜¸ì¶œ
            try:
                result = super().__call__(features)
                
                # collator í˜¸ì¶œ í›„ì—ë„ ê²°ê³¼ ê²€ì¦ (ë§¤ìš° ê°•ë ¥í•˜ê²Œ)
                if isinstance(result, dict) and 'input_ids' in result:
                    result_ids = result['input_ids']
                    if isinstance(result_ids, torch.Tensor):
                        # GPUì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ CPUë¡œ ì´ë™í•´ì„œ ê²€ì¦
                        result_ids_cpu = result_ids.cpu().numpy()
                        # ëª¨ë“  í† í°ì„ ë¨¼ì € ë²”ìœ„ë¡œ í´ë¦¬í•‘ (ì•ˆì „í•˜ê²Œ)
                        result_ids_cpu = np.clip(result_ids_cpu, 0, vocab_size - 1)
                        # ì—¬ì „íˆ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” í† í°ì´ ìˆìœ¼ë©´ pad_token_idë¡œ êµì²´
                        invalid_mask = (result_ids_cpu < 0) | (result_ids_cpu >= vocab_size)
                        if np.any(invalid_mask):
                            result_ids_cpu[invalid_mask] = self.pad_token_id
                        result['input_ids'] = torch.tensor(result_ids_cpu, dtype=result_ids.dtype, device=result_ids.device)
                
                return result
            except Exception as e:
                error_msg = str(e)
                if "CUDA" in error_msg or "device-side assert" in error_msg.lower():
                    print(f"  âš ï¸ DataCollatorì—ì„œ CUDA ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
                    print(f"  ğŸ’¡ í† í° ID ê²€ì¦ì„ ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                    # ëª¨ë“  featureë¥¼ ë‹¤ì‹œ ê²€ì¦
                    for feature in features:
                        if 'input_ids' in feature:
                            ids = feature['input_ids']
                            if isinstance(ids, (list, torch.Tensor)):
                                if isinstance(ids, torch.Tensor):
                                    ids = ids.cpu().numpy()
                                else:
                                    ids = np.array(ids)
                                # ê°•ì œë¡œ ëª¨ë“  í† í°ì„ ë²”ìœ„ ë‚´ë¡œ í´ë¦¬í•‘
                                ids = np.clip(ids, 0, vocab_size - 1)
                                if isinstance(feature['input_ids'], torch.Tensor):
                                    feature['input_ids'] = torch.tensor(ids, dtype=feature['input_ids'].dtype, device=feature['input_ids'].device)
                                else:
                                    feature['input_ids'] = ids.tolist()
                    # ë‹¤ì‹œ ì‹œë„
                    return super().__call__(features)
                else:
                    raise
    
    data_collator = SafeDataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LMì´ë¯€ë¡œ False
        model_vocab_size=model.config.vocab_size  # ëª¨ë¸ì˜ vocab_size ì „ë‹¬
    )
    
    # í•™ìŠµ ìŠ¤í… ìˆ˜ ê³„ì‚°
    effective_batch_size = batch_size * gradient_accumulation_steps
    steps_per_epoch = len(tokenized_dataset) // effective_batch_size
    if steps_per_epoch == 0:
        steps_per_epoch = 1
    total_steps = steps_per_epoch * epochs
    
    if len(tokenized_dataset) == 0:
        raise ValueError("âŒ í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    
    if len(tokenized_dataset) < effective_batch_size:
        print(f"\nâš ï¸ ê²½ê³ : ë°ì´í„°ì…‹ í¬ê¸°({len(tokenized_dataset)})ê°€ ìœ íš¨ ë°°ì¹˜ í¬ê¸°({effective_batch_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
    
    # í•™ìŠµ ì¸ì ì„¤ì •
    # output_dirì´ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ì§€ í™•ì¸
    if not output_dir:
        raise ValueError(f"output_dirì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {output_dir}")
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    model_output_dir = f"{output_dir}_{timestamp}"
    
    # ê²½ë¡œ ìœ íš¨ì„± í™•ì¸
    if not model_output_dir:
        raise ValueError(f"model_output_dirì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. output_dir={output_dir}")
    
    training_args = TrainingArguments(
        output_dir=model_output_dir,
        overwrite_output_dir=True,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        warmup_steps=min(100, max(1, total_steps // 10)),
        logging_steps=1,
        save_steps=max(10, total_steps // 5),
        save_total_limit=2,
        prediction_loss_only=True,
        remove_unused_columns=False,
        # FP16 ê´€ë ¨ ì„¤ì • ìˆ˜ì •: gradient scaling ë¬¸ì œ í•´ê²°
        fp16=False,  # FP16 ë¹„í™œì„±í™” (ì•ˆì •ì„± ìš°ì„ )
        bf16=False,  # bfloat16ë„ ë¹„í™œì„±í™”
        dataloader_pin_memory=(device == "cuda"),
        # gradient clipping ì™„ì „íˆ ë¹„í™œì„±í™” (FP16 ë¬¸ì œ í•´ê²°)
        max_grad_norm=None,  # Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ gradient clipping ë¹„í™œì„±í™”
        report_to="none",
        # Accelerate ì„¤ì • ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
        dataloader_num_workers=0,  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™” (ì•ˆì •ì„±)
    )
    
    print(f"[í•™ìŠµ ì„¤ì •]")
    print(f"  - ë””ë°”ì´ìŠ¤: {device}")
    print(f"  - ë°ì´í„°ì…‹ í¬ê¸°: {len(tokenized_dataset)}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  - ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {gradient_accumulation_steps}")
    print(f"  - ì—í¬í¬: {epochs}")
    print(f"  - ìœ íš¨ ë°°ì¹˜ í¬ê¸°: {effective_batch_size}")
    print(f"  - ìŠ¤í…/ì—í¬í¬: {steps_per_epoch}")
    print(f"  - ì˜ˆìƒ ì´ ìŠ¤í…: {total_steps}")
    
    # í•™ìŠµ ì‹œì‘ ì „ ìµœì¢… ë°ì´í„° ê²€ì¦ (CUDA ì˜¤ë¥˜ ë°©ì§€)
    print(f"\n[4/5] Trainer ì„¤ì • ë° ìµœì¢… ê²€ì¦ ì¤‘...")
    
    # ìµœì¢… ë°ì´í„° ê²€ì¦: ëª¨ë“  ìƒ˜í”Œì˜ í† í° IDê°€ ìœ íš¨í•œì§€ í™•ì¸
    print(f"  - í•™ìŠµ ì‹œì‘ ì „ ìµœì¢… ë°ì´í„° ê²€ì¦ ì¤‘...")
    final_invalid_count = 0
    vocab_size = model.config.vocab_size
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    
    # pad_token_idê°€ ìœ íš¨í•œì§€ í™•ì¸
    if pad_token_id >= vocab_size or pad_token_id < 0:
        pad_token_id = tokenizer.eos_token_id
        if pad_token_id >= vocab_size or pad_token_id < 0:
            raise ValueError(f"pad_token_idì™€ eos_token_id ëª¨ë‘ vocab_size ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤!")
    
    for i in range(len(tokenized_dataset)):
        sample = tokenized_dataset[i]
        if 'input_ids' in sample:
            input_ids = sample['input_ids']
            if input_ids:
                ids_array = np.array(input_ids, dtype=np.int64)
                # ê°•ì œë¡œ ëª¨ë“  í† í°ì„ ë²”ìœ„ ë‚´ë¡œ í´ë¦¬í•‘
                ids_array = np.clip(ids_array, 0, vocab_size - 1)
                invalid_mask = (ids_array < 0) | (ids_array >= vocab_size)
                if np.any(invalid_mask):
                    final_invalid_count += 1
                    ids_array[invalid_mask] = pad_token_id
                tokenized_dataset[i]['input_ids'] = ids_array.tolist()
    
    if final_invalid_count > 0:
        print(f"  âš ï¸ ìµœì¢… ê²€ì¦ì—ì„œ {final_invalid_count}ê°œ ìƒ˜í”Œ ìˆ˜ì •")
    else:
        print(f"  âœ… ìµœì¢… ê²€ì¦ ì™„ë£Œ: ëª¨ë“  ìƒ˜í”Œì´ ìœ íš¨í•©ë‹ˆë‹¤")
    
    # Accelerate mixed precision ëª…ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™” (FP16 ë¬¸ì œ ë°©ì§€)
    os.environ["ACCELERATE_MIXED_PRECISION"] = "no"
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    print(f"âœ… Trainer ì„¤ì • ì™„ë£Œ")
    print(f"  - FP16: {training_args.fp16}")
    print(f"  - Max Grad Norm: {training_args.max_grad_norm}")
    print(f"  - Vocab Size: {vocab_size}")
    print(f"  - Pad Token ID: {pad_token_id}")
    print()
    
    # í•™ìŠµ ì‹œì‘
    print(f"[5/5] í•™ìŠµ ì‹œì‘...")
    print(f"{'='*80}")
    print(f"ğŸ“Š í•™ìŠµ ì •ë³´:")
    print(f"   - ë°ì´í„°ì…‹: {len(tokenized_dataset)}ê°œ")
    print(f"   - ì´ ìŠ¤í…: {total_steps} (ì—í¬í¬ {epochs} Ã— ìŠ¤í…/ì—í¬í¬ {steps_per_epoch})")
    print(f"   - ë””ë°”ì´ìŠ¤: {device}")
    print(f"{'='*80}\n")
    
    train_result = None
    try:
        train_result = trainer.train()
        print(f"{'='*80}\n")
    except Exception as e:
        error_msg = str(e)
        print(f"\n{'='*80}")
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        print(f"{'='*80}")
        
        # CUDA ì˜¤ë¥˜ì¸ì§€ í™•ì¸
        is_cuda_error = "CUDA" in error_msg or "cuda" in error_msg.lower() or "device-side assert" in error_msg.lower()
        
        if is_cuda_error:
            if GPU_ONLY and not FORCE_CPU:
                # GPU ì „ìš© ëª¨ë“œì—ì„œëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨
                raise RuntimeError(
                    f"âŒ CUDA ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}\n"
                    "ğŸ’¡ GPU ì „ìš© ëª¨ë“œ(GPU_ONLY=True)ì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n"
                    "ğŸ”§ í•´ê²° ë°©ë²•:\n"
                    "   1. Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•):\n"
                    "      ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\n"
                    "   2. CPU ëª¨ë“œë¡œ í•™ìŠµí•˜ë ¤ë©´ ì½”ë“œ ìƒë‹¨ì—ì„œ FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”."
                )
            else:
                print(f"\nğŸ’¡ CUDA ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. GPUê°€ ì˜¤ì—¼ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print(f"ğŸ”§ í•´ê²° ë°©ë²•:")
                print(f"   1. Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•):")
                print(f"      ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘")
                print(f"   2. ì½”ë“œ ìƒë‹¨ì—ì„œ FORCE_CPU = Trueë¡œ ì„¤ì • í›„ ì¬ì‹¤í–‰")
                print(f"      (í•™ìŠµì€ ëŠë¦¬ì§€ë§Œ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤)")
                print(f"   3. ë‹¤ìŒ foldë¶€í„° CPU ëª¨ë“œë¡œ ìë™ ì „í™˜ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼)")
                print(f"\nâš ï¸ GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤...")
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
        
        import traceback
        traceback.print_exc()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ - ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (ì˜¤ë¥˜ ë¬´ì‹œ)
        if torch.cuda.is_available():
            try:
                if os.environ.get('CUDA_VISIBLE_DEVICES') != '':
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            except Exception as cleanup_error:
                # GPU ì •ë¦¬ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                pass
        
        # í•™ìŠµ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        return None
    
    # ëª¨ë¸ ì €ì¥ (í•™ìŠµ ì„±ê³µ ì‹œì—ë§Œ)
    if train_result is None:
        print(f"âš ï¸ í•™ìŠµì´ ì‹¤íŒ¨í•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    if not model_output_dir:
        print(f"âš ï¸ ëª¨ë¸ ì €ì¥ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    # ê²½ë¡œ ìµœì¢… í™•ì¸
    if not model_output_dir or not isinstance(model_output_dir, (str, Path)):
        error_msg = f"ëª¨ë¸ ì €ì¥ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_output_dir} (type: {type(model_output_dir)})"
        print(f"âŒ {error_msg}")
        return None
    
    # ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (Path ê°ì²´ì¸ ê²½ìš°)
    model_output_dir_str = str(model_output_dir)
    
    try:
        print(f"ëª¨ë¸ ì €ì¥ ì¤‘: {model_output_dir_str}")
        
        # trainer.save_model()ì— ëª…ì‹œì ìœ¼ë¡œ ê²½ë¡œ ì§€ì • (ì•ˆì „í•˜ê²Œ)
        # training_args.output_dirì´ Noneì´ì–´ë„ ëª…ì‹œì  ê²½ë¡œë¡œ ì €ì¥ ê°€ëŠ¥
        trainer.save_model(output_dir=model_output_dir_str)
        
        # tokenizer ì €ì¥
        tokenizer.save_pretrained(model_output_dir_str)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_output_dir_str}\n")
    except Exception as save_error:
        error_msg = str(save_error)
        print(f"âš ï¸ ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {error_msg}")
        
        # ê²½ë¡œ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
        if "NoneType" in error_msg or "PathLike" in error_msg:
            print(f"  âŒ ê²½ë¡œ ì˜¤ë¥˜ ìƒì„¸:")
            print(f"     - model_output_dir: {model_output_dir} (type: {type(model_output_dir)})")
            print(f"     - model_output_dir_str: {model_output_dir_str}")
            print(f"     - training_args.output_dir: {getattr(training_args, 'output_dir', 'N/A')}")
            print(f"     - trainer.args.output_dir: {getattr(trainer.args, 'output_dir', 'N/A') if hasattr(trainer, 'args') else 'N/A'}")
        
        # ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²½ë¡œëŠ” ë°˜í™˜ (ë¶€ë¶„ ì €ì¥ ê°€ëŠ¥)
        if model_output_dir_str and os.path.exists(model_output_dir_str):
            print(f"  ğŸ’¡ ì¼ë¶€ íŒŒì¼ì€ ì €ì¥ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤: {model_output_dir_str}")
        else:
            print(f"  âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
    
    # í•™ìŠµ ê²°ê³¼ ì¶œë ¥
    print(f"[í•™ìŠµ ì™„ë£Œ]")
    if train_result:
        print(f"  - í•™ìŠµ ì†ì‹¤: {train_result.training_loss:.4f}")
        print(f"  - ì´ í•™ìŠµ ì‹œê°„: {train_result.metrics.get('train_runtime', 0):.2f}ì´ˆ")
    print(f"  - ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_output_dir}")
    
    return model_output_dir


def run_kfold_training(data: List[Dict], k_folds: int = 5) -> List[str]:
    """
    k-fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    
    Args:
        data: í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        k_folds: fold ê°œìˆ˜
    
    Returns:
        ê° foldì˜ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\n{'='*80}")
    print(f"[k-fold êµì°¨ ê²€ì¦ ì‹œì‘]")
    print(f"  - ë°ì´í„° ê°œìˆ˜: {len(data)}")
    print(f"  - Fold ê°œìˆ˜: {k_folds}")
    print(f"{'='*80}\n")
    
    if len(data) < k_folds:
        raise ValueError(f"ë°ì´í„° ê°œìˆ˜({len(data)})ê°€ fold ê°œìˆ˜({k_folds})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤.")
    
    # ì´ˆê¸° GPU ìƒíƒœ í™•ì¸ (Colab í™˜ê²½ ê³ ë ¤)
    # GPU ìš°ì„  ì‚¬ìš©, ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë§Œ CPUë¡œ ì „í™˜
    gpu_available = False
    gpu_quota_exceeded = False
    
    # CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°ë§Œ CPU ì‚¬ìš©
    if FORCE_CPU:
        print(f"\n{'='*80}")
        print(f"ğŸ’¡ CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰í•©ë‹ˆë‹¤ (FORCE_CPU=True)")
        print(f"{'='*80}\n")
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        gpu_available = False
    elif not torch.cuda.is_available():
        # GPUê°€ ê°ì§€ë˜ì§€ ì•ŠëŠ” ê²½ìš°
        if GPU_ONLY:
            raise RuntimeError(
                "âŒ GPUê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. GPU ì „ìš© ëª¨ë“œ(GPU_ONLY=True)ì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n"
                "ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                "   1. Colabì—ì„œ GPU ëŸ°íƒ€ì„ ì„ íƒ: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: GPU\n"
                "   2. GPU í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ì‹œê°„ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\n"
                "   3. CPU ëª¨ë“œë¡œ í•™ìŠµí•˜ë ¤ë©´ ì½”ë“œ ìƒë‹¨ì—ì„œ FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”."
            )
        else:
            print(f"\nâš ï¸ GPUê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"ğŸ’¡ CPU ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤...\n")
            gpu_available = False
            gpu_quota_exceeded = True
    else:
        # GPU ì‚¬ìš© ê°€ëŠ¥
        try:
            device_count = torch.cuda.device_count()
            if device_count > 0:
                print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥ (ë””ë°”ì´ìŠ¤ {device_count}ê°œ ê°ì§€)")
                print(f"ğŸ’¡ GPU ëª¨ë“œë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                gpu_available = True
            else:
                if GPU_ONLY:
                    raise RuntimeError("GPU ë””ë°”ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. GPU ì „ìš© ëª¨ë“œì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                else:
                    gpu_available = False
                    os.environ['CUDA_VISIBLE_DEVICES'] = ''
        except Exception as e:
            if GPU_ONLY:
                raise RuntimeError(
                    f"âŒ GPU í™•ì¸ ì‹¤íŒ¨: {e}\n"
                    "ğŸ’¡ GPU ì „ìš© ëª¨ë“œ(GPU_ONLY=True)ì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n"
                    "ğŸ”§ í•´ê²° ë°©ë²•:\n"
                    "   1. Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\n"
                    "   2. CPU ëª¨ë“œë¡œ í•™ìŠµí•˜ë ¤ë©´ ì½”ë“œ ìƒë‹¨ì—ì„œ FORCE_CPU = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”."
                )
            else:
                print(f"âš ï¸ GPU í™•ì¸ ì‹¤íŒ¨: {e}")
                print(f"ğŸ’¡ CPU ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.\n")
                gpu_available = False
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    model_paths = []
    
    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(data), 1):
        print(f"\n{'='*80}")
        print(f"[Fold {fold_idx}/{k_folds}]")
        print(f"  - Train: {len(train_indices)}ê°œ")
        print(f"  - Test: {len(test_indices)}ê°œ")
        print(f"{'='*80}\n")
        
        fold_start_time = time.time()
        
        # ê° fold ì „ì— GPU ìƒíƒœ ì¬í™•ì¸ (GPU ì „ìš© ëª¨ë“œ)
        if gpu_available and os.environ.get('CUDA_VISIBLE_DEVICES') != '':
            if not torch.cuda.is_available():
                if GPU_ONLY:
                    raise RuntimeError(
                        f"âŒ Fold {fold_idx}: GPUê°€ ë” ì´ìƒ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
                        "ğŸ’¡ GPU ì „ìš© ëª¨ë“œ(GPU_ONLY=True)ì´ë¯€ë¡œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n"
                        "ğŸ”§ í•´ê²° ë°©ë²•: Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘"
                    )
                else:
                    print(f"\nâš ï¸ Fold {fold_idx}: GPUê°€ ë” ì´ìƒ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    gpu_available = False
                    os.environ['CUDA_VISIBLE_DEVICES'] = ''
            else:
                # GPU ë©”ëª¨ë¦¬ë§Œ ì •ë¦¬ (ê°„ë‹¨í•œ ì—°ì‚°ì€ í”¼í•¨)
                try:
                    torch.cuda.empty_cache()
                except:
                    pass  # GPU ì •ë¦¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # Train/Test ë°ì´í„° ë¶„í• 
        train_data = [data[i] for i in train_indices]
        test_data = [data[i] for i in test_indices]
        
        # ëª¨ë¸ í•™ìŠµ
        try:
            fold_output_dir = f"{OUTPUT_DIR}_fold{fold_idx}"
            model_path = train_kogpt2_model(
                train_data=train_data,
                output_dir=fold_output_dir,
                epochs=EPOCHS,
                learning_rate=LEARNING_RATE,
                batch_size=BATCH_SIZE,
                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS
            )
            model_paths.append(model_path)
            
            fold_time = time.time() - fold_start_time
            print(f"\nâœ… [Fold {fold_idx} ì™„ë£Œ]")
            print(f"  - ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")
            print(f"  - ì†Œìš” ì‹œê°„: {fold_time:.2f}ì´ˆ")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë‹¤ìŒ foldë¥¼ ìœ„í•´)
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    print(f"  - GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    print(f"  âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        except Exception as e:
            print(f"\nâŒ [Fold {fold_idx} ì‹¤íŒ¨]: {e}")
            import traceback
            traceback.print_exc()
            model_paths.append(None)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì˜¤ë¥˜ í›„ì—ë„) - ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    # GPU ìƒíƒœ ì´ˆê¸°í™” ì‹œë„
                    try:
                        torch.cuda.reset_peak_memory_stats()
                    except:
                        pass
                except Exception as cleanup_error:
                    print(f"  âš ï¸ GPU ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {cleanup_error}")
                    print(f"  ğŸ’¡ GPUê°€ ì˜¤ì—¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ foldë¶€í„° CPU ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
                    print(f"     ğŸ’¡ ë” ë¹ ë¥¸ í•™ìŠµì„ ì›í•˜ì‹œë©´ Colab ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”:")
                    print(f"        ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘")
                    # ë‹¤ìŒ foldë¶€í„° GPU ì‚¬ìš© ì•ˆ í•˜ë„ë¡ ì„¤ì •
                    os.environ['CUDA_VISIBLE_DEVICES'] = ''
                    gpu_available = False
    
    return model_paths


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"\n{'='*80}")
    print("Google Colabì—ì„œ koGPT2 ëª¨ë¸ k-fold êµì°¨ ê²€ì¦ í•™ìŠµ")
    print(f"{'='*80}\n")
    
    # GPU í™•ì¸ (Colab í™˜ê²½) - GPU ìš°ì„  ì‚¬ìš©
    if FORCE_CPU:
        print("ğŸ’¡ CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰í•©ë‹ˆë‹¤ (FORCE_CPU=True)")
        print("   CUDA ì˜¤ë¥˜ë¥¼ í”¼í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   âš ï¸ CPU ëª¨ë“œëŠ” ë§¤ìš° ëŠë¦¬ì§€ë§Œ ì•ˆì •ì ì…ë‹ˆë‹¤.")
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
    elif torch.cuda.is_available():
        try:
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name}")
            print("ğŸ’¡ GPU ëª¨ë“œë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            print("   CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘")
        except Exception as e:
            print(f"âš ï¸ GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            print("ğŸ’¡ GPUê°€ ì˜¤ì—¼ë˜ì—ˆê±°ë‚˜ í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("   CPU ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
            print("   ğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.")
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
    else:
        print("âš ï¸ GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("\nğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:")
        print("   ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: GPU")
        print("\nâš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼).")
        print("ğŸ’¡ ê°€ëŠ¥í•œ ì›ì¸:")
        print("   1. Colab GPU í• ë‹¹ëŸ‰ ì†Œì§„ (ë¬´ë£Œ ë²„ì „ì€ ì‹œê°„ ì œí•œì´ ìˆìŒ)")
        print("   2. GPU ëŸ°íƒ€ì„ì´ ì„ íƒë˜ì§€ ì•ŠìŒ")
        print("\nğŸ”§ í•´ê²° ë°©ë²•:")
        print("   1. ëª‡ ì‹œê°„ í›„ ë‹¤ì‹œ ì‹œë„ (í• ë‹¹ëŸ‰ ë³µêµ¬ ëŒ€ê¸°)")
        print("   2. Colab Pro/Pro+ êµ¬ë…")
        print("   3. CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰ (ë§¤ìš° ëŠë¦¼)")
    
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    try:
        data = download_kpoem_data(max_size=MAX_DATA_SIZE)
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    if len(data) == 0:
        print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # k-fold êµì°¨ ê²€ì¦ í•™ìŠµ
    try:
        model_paths = run_kfold_training(data, k_folds=K_FOLDS)
        
        print(f"\n{'='*80}")
        print("âœ… k-fold êµì°¨ ê²€ì¦ í•™ìŠµ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"\nğŸ“¦ í•™ìŠµëœ ëª¨ë¸ ìœ„ì¹˜:")
        for fold_idx, model_path in enumerate(model_paths, 1):
            if model_path:
                print(f"  - Fold {fold_idx}: {model_path}")
            else:
                print(f"  - Fold {fold_idx}: ì‹¤íŒ¨")
        
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"1. Google Driveì— ì—…ë¡œë“œ:")
        print(f"   - ê° foldì˜ ëª¨ë¸ í´ë”ë¥¼ Google Driveì— ì—…ë¡œë“œ")
        print(f"2. ë˜ëŠ” ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ:")
        print(f"   - Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì‚¬ìš©")
        print(f"   - ë˜ëŠ” zipìœ¼ë¡œ ì••ì¶• í›„ ë‹¤ìš´ë¡œë“œ")
        print(f"\nğŸ“ ë¡œì»¬ì—ì„œ ì‚¬ìš© ë°©ë²•:")
        print(f"   - í•™ìŠµëœ ëª¨ë¸ì„ 'backend/trained_models/' í´ë”ì— ë³µì‚¬")
        print(f"   - kfold_poem_generation.pyì—ì„œ ì‚¬ìš©")
        print(f"\nğŸ’¡ ëª¨ë“  fold ëª¨ë¸ì„ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ:")
        print(f"   - ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ zipìœ¼ë¡œ ì••ì¶•:")
        print(f"   ```python")
        print(f"   import shutil")
        print(f"   from google.colab import files")
        print(f"   shutil.make_archive('all_folds_models', 'zip', '{OUTPUT_DIR}')")
        print(f"   files.download('all_folds_models.zip')")
        print(f"   ```")
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

